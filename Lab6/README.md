# ğŸ§  Lab 6: Giá»›i thiá»‡u vá» Transformers

## 1. Má»¥c tiÃªu
- Ã”n táº­p kiáº¿n thá»©c cÆ¡ báº£n vá» kiáº¿n trÃºc Transformer.
- Sá»­ dá»¥ng cÃ¡c mÃ´ hÃ¬nh Transformer tiá»n huáº¥n luyá»‡n cho cÃ¡c tÃ¡c vá»¥ NLP cÆ¡ báº£n.
- LÃ m quen vá»›i thÆ° viá»‡n **Hugging Face Transformers**.

---

## 2. Kiáº¿n thá»©c cÆ¡ báº£n vá» Transformers

### 2.1 Kiáº¿n trÃºc Transformer
Transformer gá»“m hai thÃ nh pháº§n chÃ­nh:

- **Encoder**: MÃ£ hÃ³a vÄƒn báº£n Ä‘áº§u vÃ o thÃ nh cÃ¡c vector ngá»¯ cáº£nh.
- **Decoder**: Sinh vÄƒn báº£n Ä‘áº§u ra dá»±a trÃªn biá»ƒu diá»…n cá»§a Encoder vÃ  cÃ¡c token trÆ°á»›c Ä‘Ã³.
- **Self-Attention**: Cho phÃ©p mÃ´ hÃ¬nh Ä‘Ã¡nh giÃ¡ má»©c Ä‘á»™ quan trá»ng cá»§a cÃ¡c tá»« trong cÃ¢u.

### 2.2 CÃ¡c loáº¡i mÃ´ hÃ¬nh Transformer
- **Encoder-only**: BERT, RoBERTa â†’ hiá»ƒu ngá»¯ cáº£nh, phÃ¢n loáº¡i, NER, MLM.
- **Decoder-only**: GPT â†’ sinh vÄƒn báº£n, dá»± Ä‘oÃ¡n token tiáº¿p theo.
- **Encoderâ€“Decoder**: T5, BART â†’ dá»‹ch mÃ¡y, tÃ³m táº¯t.

---

## 3. CÃ i Ä‘áº·t mÃ´i trÆ°á»ng

```bash
pip install transformers torch
```

---

## 4. BÃ i táº­p thá»±c hÃ nh

### BÃ i 1: Masked Language Modeling (MLM)

**Code:**
```python
from transformers import pipeline

mask_filler = pipeline("fill-mask")

input_sentence = "Hanoi is the [MASK] of Vietnam."
predictions = mask_filler(input_sentence, top_k=5)

print(f"CÃ¢u gá»‘c: {input_sentence}")
for pred in predictions:
    print(f"Dá»± Ä‘oÃ¡n: '{pred['token_str']}' | score={pred['score']:.4f}")
    print(f" -> {pred['sequence']}")
```

**CÃ¢u há»i & tráº£ lá»i:**

1. **MÃ´ hÃ¬nh cÃ³ dá»± Ä‘oÃ¡n Ä‘Ãºng tá»« *capital* khÃ´ng?**  
â†’ CÃ³. Trong háº§u háº¿t cÃ¡c láº§n cháº¡y, tá»« *capital* xuáº¥t hiá»‡n á»Ÿ vá»‹ trÃ­ Ä‘áº§u hoáº·c trong top-5 vá»›i Ä‘á»™ tin cáº­y cao.

2. **VÃ¬ sao Encoder-only (BERT) phÃ¹ há»£p cho tÃ¡c vá»¥ nÃ y?**  
â†’ VÃ¬ BERT cÃ³ kháº£ nÄƒng nhÃ¬n ngá»¯ cáº£nh **hai chiá»u**, cho phÃ©p táº­n dá»¥ng cáº£ tá»« trÆ°á»›c vÃ  sau token bá»‹ che Ä‘á»ƒ dá»± Ä‘oÃ¡n chÃ­nh xÃ¡c.

---

### BÃ i 2: Next Token Prediction (Text Generation)

**Code:**
```python
from transformers import pipeline

generator = pipeline("text-generation")

prompt = "The best thing about learning NLP is"
outputs = generator(prompt, max_length=50, num_return_sequences=1)

print(f"CÃ¢u má»“i: {prompt}")
print(outputs[0]['generated_text'])
```

**CÃ¢u há»i & tráº£ lá»i:**

1. **Káº¿t quáº£ sinh ra cÃ³ há»£p lÃ½ khÃ´ng?**  
â†’ CÃ³. VÄƒn báº£n sinh ra thÆ°á»ng máº¡ch láº¡c vÃ  liÃªn quan Ä‘áº¿n há»c NLP.

2. **VÃ¬ sao Decoder-only (GPT) phÃ¹ há»£p?**  
â†’ GPT Ä‘Æ°á»£c huáº¥n luyá»‡n Ä‘á»ƒ dá»± Ä‘oÃ¡n **token tiáº¿p theo** theo hÆ°á»›ng má»™t chiá»u, ráº¥t phÃ¹ há»£p cho sinh vÄƒn báº£n liÃªn tá»¥c.

---

### BÃ i 3: Vector biá»ƒu diá»…n cÃ¢u (Sentence Representation)

**Code (Mean Pooling):**
```python
import torch
from transformers import AutoTokenizer, AutoModel

model_name = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

sentences = ["This is a sample sentence."]
inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')

with torch.no_grad():
    outputs = model(**inputs)

last_hidden_state = outputs.last_hidden_state
attention_mask = inputs['attention_mask']

mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()
sum_embeddings = torch.sum(last_hidden_state * mask_expanded, 1)
sum_mask = torch.clamp(mask_expanded.sum(1), min=1e-9)
sentence_embedding = sum_embeddings / sum_mask

print(sentence_embedding)
print("KÃ­ch thÆ°á»›c vector:", sentence_embedding.shape)
```

**CÃ¢u há»i & tráº£ lá»i:**

1. **KÃ­ch thÆ°á»›c vector lÃ  bao nhiÃªu? TÆ°Æ¡ng á»©ng tham sá»‘ nÃ o?**  
â†’ Vector cÃ³ kÃ­ch thÆ°á»›c **(1, 768)**.  
â†’ 768 lÃ  **hidden_size** cá»§a mÃ´ hÃ¬nh `bert-base-uncased`.

2. **VÃ¬ sao cáº§n attention_mask khi Mean Pooling?**  
â†’ Äá»ƒ **loáº¡i bá» áº£nh hÆ°á»Ÿng cá»§a cÃ¡c token padding**, Ä‘áº£m báº£o chá»‰ tÃ­nh trung bÃ¬nh trÃªn cÃ¡c token tháº­t.

---

## 5. Tá»•ng káº¿t

- BERT ráº¥t máº¡nh cho cÃ¡c tÃ¡c vá»¥ **hiá»ƒu ngá»¯ cáº£nh** (MLM, embedding).
- GPT phÃ¹ há»£p cho **sinh vÄƒn báº£n** vÃ  dá»± Ä‘oÃ¡n token tiáº¿p theo.
- Mean Pooling táº¡o sentence embedding á»•n Ä‘á»‹nh vÃ  hiá»‡u quáº£.
- Transformer lÃ  ná»n táº£ng cho cÃ¡c mÃ´ hÃ¬nh NLP hiá»‡n Ä‘áº¡i nhÆ° BERT, GPT, T5.

---

## 6. Káº¿t luáº­n

Lab nÃ y giÃºp lÃ m quen vá»›i:
- Kiáº¿n trÃºc Transformer
- Hugging Face pipelines
- á»¨ng dá»¥ng thá»±c táº¿ cá»§a Encoder-only vÃ  Decoder-only models

ÄÃ¢y lÃ  ná»n táº£ng quan trá»ng Ä‘á»ƒ tiáº¿p cáº­n cÃ¡c mÃ´ hÃ¬nh NLP nÃ¢ng cao.

